{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4295b2-d06b-4d92-b643-7d4d8f35eae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ce06a2-96de-4ed1-b0e3-43c52d10893d",
   "metadata": {},
   "source": [
    "# Parte 1 – Introducción al Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b431fd5-088b-4561-9b87-f1f40079e2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import statistics\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0b114a-1477-4b66-a5f9-4dc7a399bb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"mlflow_basico\")\n",
    "\n",
    "def modelo_promedio(valores):\n",
    "    return statistics.mean(valores)\n",
    "\n",
    "with mlflow.start_run(run_name=\"ejemplo_promedio\"):\n",
    "\n",
    "    lista_valores = [random.randint(1, 100) for _ in range(10)]\n",
    "    mlflow.log_param(\"tamaño_lista\", len(lista_valores))\n",
    "    mlflow.log_param(\"valores\", lista_valores)\n",
    "\n",
    "    resultado = modelo_promedio(lista_valores)\n",
    "    mlflow.log_metric(\"promedio\", resultado)\n",
    "\n",
    "    with open(\"resultado.txt\", \"w\") as f:\n",
    "        f.write(f\"Valores: {lista_valores}\\n\")\n",
    "        f.write(f\"Promedio: {resultado}\\n\")\n",
    "\n",
    "    mlflow.log_artifact(\"resultado.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9517ae35-63b1-4c45-8929-624577643bf8",
   "metadata": {},
   "source": [
    "# Parte 2 – Registro de Modelos de Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56b4674-a2d9-43c9-a456-5ac65fb6801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "import logging\n",
    "from mlflow.tracking import MlflowClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153d7038-ce7d-42ad-8344-2ca7a3e89a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "mlflow.set_experiment(\"breast_cancer_experiment\")\n",
    "\n",
    "with mlflow.start_run(run_name = \"breast_cancer_logistic_regression_v1\"):\n",
    "\n",
    "    X, y = load_breast_cancer(return_X_y= True)\n",
    "    params = {\"C\": 0.1, \"random_state\": 42, \"max_iter\": 1000, \"solver\": \"liblinear\", \"penalty\": \"l2\"}\n",
    "    mlflow.log_params(params)\n",
    "    lr = LogisticRegression(**params).fit(X, y)\n",
    "    y_pred = lr.predict(X)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy_score(y, y_pred))\n",
    "    mlflow.log_metric(\"f1_score\", f1_score(y, y_pred))\n",
    "    mlflow.log_mectric(\"precision\", precision_score(y, y_pred))\n",
    "    mlflow.sklearn.log_model(lr, \"breast_cancer_logistic_regression_model\")\n",
    "    result = mlflow.register_model(\n",
    "        model_uri=f\"runs:/{run.info.run_id}/breast_cancer_logistic_regression_model\",\n",
    "        name=\"bc_logistic_regression_model\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4aaa59f-bd56-49ac-901a-cabb793e5462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Promover a Staging\n",
    "\n",
    "client = MlflowClient()\n",
    "model_name = \"bc_logistic_regression_model\"\n",
    "version = 1\n",
    "\n",
    "client.transition_model_version_stage(\n",
    "    name=model_name,\n",
    "    version=version,\n",
    "    stage=\"Staging\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2823efc-c16f-4b61-8386-4d49a52f765a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Promover a Production\n",
    "\n",
    "client.transition_model_version_stage(\n",
    "    name=model_name,\n",
    "    version=version,\n",
    "    stage=\"Production\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2254f8-be3a-4469-944a-dce9169a9380",
   "metadata": {},
   "source": [
    "# Parte 3 – Tracking de Modelos de Lenguaje (LLMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c362110a-8673-428f-8c0a-0e21d10a2a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "gemini_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    "\n",
    "gemini = OpenAI(base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\", api_key=gemini_key)\n",
    "model_name_g = \"gemini-2.5-flash-lite\"\n",
    "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "model_name_o = \"gpt-oss:20b\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e49dedc-59a7-4a58-ae9a-7ade29ff7ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import mlflow\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "load_dotenv()\n",
    "gemini_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    "\n",
    "gemini = OpenAI(base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\", api_key=gemini_key)\n",
    "ollama = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "\n",
    "# Prompt de prueba\n",
    "prompt = \"¿Que es la IA?\"\n",
    "\n",
    "# Simulación de costo por token\n",
    "def estimar_costo(tokens):\n",
    "    return tokens * 0.0001\n",
    "\n",
    "def ejecutar_llm(nombre_modelo, cliente, proveedor, temperatura, tipo_tarea, registry_name):\n",
    "    mlflow.set_experiment(\"comparacion_llm_chat\")\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"{registry_name}_run\") as run:\n",
    "        inicio = time.time()\n",
    "        response = cliente.chat.completions.create(\n",
    "            model=nombre_modelo,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperatura\n",
    "        )\n",
    "        fin = time.time()\n",
    "\n",
    "        texto = response.choices[0].message.content\n",
    "        tokens = response.usage.total_tokens\n",
    "        latencia = fin - inicio\n",
    "        costo = estimar_costo(tokens)\n",
    "\n",
    "        # Log de parámetros\n",
    "        mlflow.log_param(\"modelo\", nombre_modelo)\n",
    "        mlflow.log_param(\"temperatura\", temperatura)\n",
    "        mlflow.log_param(\"proveedor\", proveedor)\n",
    "        mlflow.log_param(\"tipo_tarea\", tipo_tarea)\n",
    "\n",
    "        # Log de métricas\n",
    "        mlflow.log_metric(\"latencia\", latencia)\n",
    "        mlflow.log_metric(\"tokens\", tokens)\n",
    "        mlflow.log_metric(\"costo_estimado\", costo)\n",
    "\n",
    "        # Artefactos\n",
    "        with open(\"prompt.txt\", \"w\") as f:\n",
    "            f.write(prompt)\n",
    "        with open(\"respuesta.txt\", \"w\") as f:\n",
    "            f.write(texto)\n",
    "\n",
    "        mlflow.log_artifact(\"prompt.txt\")\n",
    "        mlflow.log_artifact(\"respuesta.txt\")\n",
    "\n",
    "        # Registrar modelo en el Model Registry\n",
    "        mlflow.register_model(\n",
    "            model_uri=f\"runs:/{run.info.run_id}/respuesta.txt\",\n",
    "            name=registry_name\n",
    "        )\n",
    "\n",
    "# Ejecutar ambos modelos\n",
    "ejecutar_llm(\"gemini-2.5-flash-lite\", gemini, \"Gemini\", 0.7, \"chat\", \"llm_gemini_chat\")\n",
    "ejecutar_llm(\"gpt-oss:20b\", ollama, \"Ollama\", 0.7, \"chat\", \"llm_ollama_chat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a05af53-75a5-4a16-9bfa-05a778205c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Promocion de los modelos a Staging\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "client.transition_model_version_stage(\n",
    "    name=\"llm_gemini_chat\",\n",
    "    version=1,\n",
    "    stage=\"Staging\"\n",
    ")\n",
    "\n",
    "client.transition_model_version_stage(\n",
    "    name=\"llm_ollama_chat\",\n",
    "    version=1,\n",
    "    stage=\"Staging\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
